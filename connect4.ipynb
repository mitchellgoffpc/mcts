{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS in MCTX\n",
    "\n",
    "In this example, we will use the `mctx` library to play the game of Connect 4.\n",
    "We will implement the Monte Carlo Tree Search (MCTS) algorithm with random rollouts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game mechanics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some type aliases to make the code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "\n",
    "# 6x7 board\n",
    "# We use the following coordinate system:\n",
    "#   ^\n",
    "#   |\n",
    "#   |\n",
    "#   |\n",
    "# 0 +-------->\n",
    "#   0\n",
    "Board = chex.Array\n",
    "\n",
    "# Index of the column to play\n",
    "Action = chex.Array\n",
    "\n",
    "# Let's assume the game is played by players X and O.\n",
    "# 1 if player X, -1 if player O\n",
    "Player = chex.Array\n",
    "\n",
    "# Reward for the player who played the action.\n",
    "# 1 for winning, 0 for draw, -1 for losing\n",
    "Reward = chex.Array\n",
    "\n",
    "# True/False if the game is over\n",
    "Done = chex.Array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a class defining the game state at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class Env:\n",
    "    board: Board\n",
    "    player: Player\n",
    "    done: Done\n",
    "    reward: Reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the game state, we define a function that prints the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "BOARD_STRING = \"\"\"\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " ? | ? | ? | ? | ? | ? | ?\n",
    "---|---|---|---|---|---|---\n",
    " 1   2   3   4   5   6   7\n",
    "\"\"\"\n",
    "\n",
    "def print_board(board: Board):\n",
    "    board_str = BOARD_STRING\n",
    "    for i in reversed(range(board.shape[0])):\n",
    "        for j in range(board.shape[1]):\n",
    "            board_str = board_str.replace('?', '[green]X[/green]' if board[i, j] == 1 else '[red]O[/red]' if board[i, j] == -1 else ' ', 1)\n",
    "    print(board_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that checks if the game is over.\n",
    "We iterate over all horizontal/vertical/diagonal/anti-diagonal lines and check if there are 4 consecutive pieces of the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def horizontals(board: Board) -> chex.Array:\n",
    "    return jnp.stack([\n",
    "        board[i, j:j+4]\n",
    "        for i in range(board.shape[0])\n",
    "        for j in range(board.shape[1] - 3)\n",
    "    ])\n",
    "\n",
    "def verticals(board: Board) -> chex.Array:\n",
    "    return jnp.stack([\n",
    "        board[i:i+4, j]\n",
    "        for i in range(board.shape[0] - 3)\n",
    "        for j in range(board.shape[1])\n",
    "    ])\n",
    "\n",
    "def diagonals(board: Board) -> chex.Array:\n",
    "    return jnp.stack([\n",
    "        jnp.diag(board[i:i+4, j:j+4])\n",
    "        for i in range(board.shape[0] - 3)\n",
    "        for j in range(board.shape[1] - 3)\n",
    "    ])\n",
    "\n",
    "def antidiagonals(board: Board) -> chex.Array:\n",
    "    return jnp.stack([\n",
    "        jnp.diag(board[i:i+4, j:j+4][::-1])\n",
    "        for i in range(board.shape[0] - 3)\n",
    "        for j in range(board.shape[1] - 3)\n",
    "    ])\n",
    "\n",
    "def get_winner(board: Board) -> Player:\n",
    "    all_lines = jnp.concatenate((\n",
    "        horizontals(board),\n",
    "        verticals(board),\n",
    "        diagonals(board),\n",
    "        antidiagonals(board),\n",
    "    ))\n",
    "    # x_won and o_won are 1 if the player won, 0 otherwise\n",
    "    x_won = jnp.any(jnp.all(all_lines == 1, axis=1)).astype(jnp.int8)\n",
    "    o_won = jnp.any(jnp.all(all_lines == -1, axis=1)).astype(jnp.int8)\n",
    "    # We consider the following cases:\n",
    "    # - !x_won and !o_won -> 0 - 0 = 0 -> draw OR not finished\n",
    "    # - x_won and !o_won -> 1 - 0 = 1 -> Player 1 (X) won\n",
    "    # - !x_won and o_won -> 0 - 1 = -1 -> Player -1 (O) won\n",
    "    # - x_won and o_won -> impossible, the game would have ended earlier\n",
    "    return x_won - o_won"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can implement the environment dynamics:\n",
    "\n",
    "* `env_reset` creates a brand new game\n",
    "* `env_step` plays a move and returns the new state, the reward **for the player who played the move** and whether the game has ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def env_reset(_):\n",
    "    return Env(\n",
    "        board=jnp.zeros((6, 7), dtype=jnp.int8),\n",
    "        player=jnp.int8(1),\n",
    "        done=jnp.bool_(False),\n",
    "        reward=jnp.int8(0),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why `env_reset` takes an unused argument.\n",
    "This allows us to parallelize the environment with the `jax.vmap` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 16:12:21.880761: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.3 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jax.vmap(env_reset)(jnp.arange(10)).player\n",
    "# 10 games have been created:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to perfectly understand the `env_step` function to implement the MCTS algorithm.\n",
    "\n",
    "In particular, you need to understand that the reward returned by `env_step` is the reward **for the player who played the move**.\n",
    "This means that the reward should always be either 0 for a move that does not end the game or causes a draw, or 1 for a move that ends the game with a win for the player who played the move.\n",
    "\n",
    "A reward of -1 is reserved for punishing illegal moves.\n",
    "Since `mctx` does not support masking illegal moves below the root node, we need another way to stop the MCTS algorithm from exploring illegal moves.\n",
    "Later, we will also write a custom policy function that will never select illegal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(env: Env, action: Action) -> tuple[Env, Reward, Done]:\n",
    "    col = action\n",
    "\n",
    "    # Find the first empty row in the column.\n",
    "    # If the column is full, this will be the top row.\n",
    "    row = jnp.argmax(env.board[:, col] == 0)\n",
    "\n",
    "    # If the column is full, the move is invalid.\n",
    "    invalid_move = env.board[row, col] != 0\n",
    "\n",
    "    # Place the player's piece in the board only if the move is valid and the game is not over.\n",
    "    board = env.board.at[row, col].set(jnp.where(env.done | invalid_move, env.board[row, col], env.player))\n",
    "\n",
    "    # The reward is computed as follows:\n",
    "    # * 0 if the game is **already** over. This is to ignore nodes below terminal nodes.\n",
    "    # * -1 if the move is invalid\n",
    "    # * 1 if the move won the game for the current player\n",
    "    # * 0 if the move caused a draw\n",
    "    # * (impossible for Connect 4) -1 if the move lost the game for the current player\n",
    "    reward = jnp.where(env.done, 0, jnp.where(invalid_move, -1, get_winner(board) * env.player)).astype(jnp.int8)\n",
    "\n",
    "    # We end the game if:\n",
    "    # * the game was already over\n",
    "    # * the move won or lost the game\n",
    "    # * the move was invalid\n",
    "    # * the board is full (draw)\n",
    "    done = env.done | reward != 0 | invalid_move | jnp.all(board[-1] != 0)\n",
    "\n",
    "    env = Env(\n",
    "        board=board,\n",
    "        # switch player\n",
    "        player=jnp.where(done, env.player, -env.player),\n",
    "        done=done,\n",
    "        reward=reward,\n",
    "    )\n",
    "\n",
    "    return env, reward, done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MCTS algorithm\n",
    "\n",
    "We can now implement the MCTS algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_action_mask(env: Env) -> chex.Array:\n",
    "    '''\n",
    "    Computes which actions are valid in the current state.\n",
    "    Returns an array of booleans, indicating which columns are not full.\n",
    "    In case the game is over, all columns are considered invalid.\n",
    "    '''\n",
    "    return jnp.where(env.done, jnp.array([False] * env.board.shape[1]), env.board[-1] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winning_action_mask(env: Env, player: Player) -> chex.Array:\n",
    "    '''\n",
    "    Finds all actions that would immediately win the game for the given player.\n",
    "    '''\n",
    "    # Override the next player in the environment with the given player.\n",
    "    env = Env(board=env.board, player=player, done=env.done, reward=env.reward)\n",
    "\n",
    "    # Play all actions and check the reward.\n",
    "    # Remember that the reward is for the current player, so we expect it to be 1.\n",
    "    env, reward, done = jax.vmap(env_step, (None, 0))(env, jnp.arange(7, dtype=jnp.int8))\n",
    "    return reward == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a policy function. The policy function is used in two places:\n",
    "\n",
    "* during random rollouts to select the next action to play\n",
    "* during node expansion as the prior distribution\n",
    "\n",
    "We could have used a uniform distribution for both, but we can do better.\n",
    "Let's use three simple heuristics:\n",
    "\n",
    "* never select illegal moves\n",
    "* always select winning moves if available\n",
    "* always block opponent's winning moves if we have no winning move\n",
    "\n",
    "Our policy function implementation returns unnormalized logits,\n",
    "so we cannot return pure 0% and 100% probabilities (using `inf` and `-inf` causes downstream numerical issues).\n",
    "Instead, we add `100` to the logits of a given action which we want to prioritize:\n",
    "\n",
    "* the lowest priority of `0` is assigned to illegal moves\n",
    "* a medium priority of `100` is assigned to legal moves\n",
    "* a higher priority of `200` is assigned to the opponent's winning moves\n",
    "* the highest priority of `300` is assigned to our winning moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_function(env: Env) -> chex.Array:\n",
    "    # return jnp.zeros(env.board.shape[1], dtype=jnp.float32)\n",
    "    return sum((\n",
    "        valid_action_mask(env).astype(jnp.float32) * 100,\n",
    "        winning_action_mask(env, -env.player).astype(jnp.float32) * 200,\n",
    "        winning_action_mask(env, env.player).astype(jnp.float32) * 300,\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how these logits translate to probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 3.8e-44],      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env_reset(0)\n",
    "env, _, _ = env_step(env, 0)\n",
    "env, _, _ = env_step(env, 6)\n",
    "env, _, _ = env_step(env, 0)\n",
    "env, _, _ = env_step(env, 6)\n",
    "env, _, _ = env_step(env, 0)\n",
    "env, _, _ = env_step(env, 6)\n",
    "jax.nn.softmax(policy_function(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715,\n",
       "       0.14285715, 0.14285715], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nn.softmax(jnp.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.0e+00, 3.8e-44, 3.8e-44, 3.8e-44, 3.8e-44, 3.8e-44, 3.8e-44],      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nn.softmax(jnp.array([100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([3.8e-44, 1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00],      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nn.softmax(jnp.array([100.0, 200.0, 0.0, 0.0, 0.0, 0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.0e+00, 3.8e-44, 1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00],      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nn.softmax(jnp.array([100.0, 200.0, 300.0, 0.0, 0.0, 0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.e+00, 2.e-44, 5.e-01, 5.e-01, 0.e+00, 0.e+00, 0.e+00], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nn.softmax(jnp.array([100.0, 200.0, 300.0, 300.0, 0.0, 0.0, 0.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that a difference of 100 is enough to make a move almost certain to be selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MCTS algorithm uses random rollouts to estimate the value of each state.\n",
    "We can implement this with a simple function that plays moves according to our policy function until the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env: Env, rng_key: chex.PRNGKey) -> Reward:\n",
    "    '''\n",
    "    Plays a game until the end and returns the reward from the perspective of the initial player.\n",
    "    '''\n",
    "    def cond(a):\n",
    "        env, key = a\n",
    "        return ~env.done\n",
    "    def step(a):\n",
    "        env, key = a\n",
    "        key, subkey = jax.random.split(key)\n",
    "        action = jax.random.categorical(subkey, policy_function(env))\n",
    "        env, reward, done = env_step(env, action)\n",
    "        return env, key\n",
    "    leaf, key = jax.lax.while_loop(cond, step, (env, rng_key))\n",
    "    # The leaf reward is from the perspective of the last player.\n",
    "    # We negate it if the last player is not the initial player.\n",
    "    return leaf.reward * leaf.player * env.player"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function is simply the result of the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(env: Env, rng_key: chex.PRNGKey) -> chex.Array:\n",
    "    return rollout(env, rng_key).astype(jnp.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mctx\n",
    "\n",
    "Finally, we get to use the `mctx` library.\n",
    "We need two function:\n",
    "\n",
    "* `root_fn` returns the root node of the MCTS tree\n",
    "* `recurrent_fn` expands a new node given a parent node and an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mctx\n",
    "\n",
    "def root_fn(env: Env, rng_key: chex.PRNGKey) -> mctx.RootFnOutput:\n",
    "    return mctx.RootFnOutput(\n",
    "        prior_logits=policy_function(env),\n",
    "        value=value_function(env, rng_key),\n",
    "        # We will use the `embedding` field to store the environment.\n",
    "        embedding=env,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_fn(params, rng_key, action, embedding):\n",
    "    # Extract the environment from the embedding.\n",
    "    env = embedding\n",
    "\n",
    "    # Play the action.\n",
    "    env, reward, done = env_step(env, action)\n",
    "\n",
    "    # Create the new MCTS node.\n",
    "    recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "        # reward for playing `action`\n",
    "        reward=reward,\n",
    "        # discount explained in the next section\n",
    "        discount=jnp.where(done, 0, -1).astype(jnp.float32),\n",
    "        # prior for the new state\n",
    "        prior_logits=policy_function(env),\n",
    "        # value for the new state\n",
    "        value=jnp.where(done, 0, value_function(env, rng_key)).astype(jnp.float32),\n",
    "    )\n",
    "\n",
    "    # Return the new node and the new environment.\n",
    "    return recurrent_fn_output, env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `discount` field is used to flip the sign of the reward and value at each tree level.\n",
    "It would be possible to implement this in the environment dynamics, but it is more convenient to do it here.\n",
    "By flipping the sign, MCTS selects the best action for the current player of each turn.\n",
    "\n",
    "Note that we set `discount` to `0` when the game is over.\n",
    "This discards all the rewards and values after the end of the game."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run MCTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2,))\n",
    "def run_mcts(rng_key: chex.PRNGKey, env: Env, num_simulations: int) -> chex.Array:\n",
    "    batch_size = 1\n",
    "    key1, key2 = jax.random.split(rng_key)\n",
    "    policy_output = mctx.muzero_policy(\n",
    "        # params can be used to pass additional data to the recurrent_fn like neural network weights\n",
    "        params=None,\n",
    "\n",
    "        rng_key=key1,\n",
    "\n",
    "        # create a batch of environments (in this case, a batch of size 1)\n",
    "        root=jax.vmap(root_fn, (None, 0))(env, jax.random.split(key2, batch_size)),\n",
    "\n",
    "        # automatically vectorize the recurrent_fn\n",
    "        recurrent_fn=jax.vmap(recurrent_fn, (None, None, 0, 0)),\n",
    "\n",
    "        num_simulations=num_simulations,\n",
    "\n",
    "        # we limit the depth of the search tree to 42, since we know that Connect Four can't last longer\n",
    "        max_depth=42,\n",
    "\n",
    "        # our value is in the range [-1, 1], so we can use the min_max qtransform to map it to [0, 1]\n",
    "        qtransform=functools.partial(mctx.qtransform_by_min_max, min_value=-1, max_value=1),\n",
    "\n",
    "        # Dirichlet noise is used for exploration which we don't need in this example (we aren't training)\n",
    "        dirichlet_fraction=0.0,\n",
    "    )\n",
    "    return policy_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play the middle column two times and see what MCTS does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.072</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.238</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.193</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11400001</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29000002</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.066</span>\n",
       "  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027</span>     <span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.072\u001b[0m      \u001b[1;36m0.238\u001b[0m      \u001b[1;36m0.193\u001b[0m      \u001b[1;36m0.11400001\u001b[0m \u001b[1;36m0.29000002\u001b[0m \u001b[1;36m0.066\u001b[0m\n",
       "  \u001b[1;36m0.027\u001b[0m     \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = env_reset(0)\n",
    "env, reward, done = env_step(env, 3)\n",
    "env, reward, done = env_step(env, 3)\n",
    "policy_output = run_mcts(jax.random.PRNGKey(38), env, 1000)\n",
    "print(policy_output.action_weights)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(jnp.arange(7), policy_output.action_weights.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It chose the middle column as the best move, as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simple script to play against MCTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False to enable human input\n",
    "player_1_ai = True\n",
    "player_2_ai = True\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "env = env_reset(0)\n",
    "print_board(env.board)\n",
    "while True:\n",
    "    if player_1_ai:\n",
    "        action = run_mcts(key, env, 1000).action_weights.argmax().item()\n",
    "    else:\n",
    "        action = int(input()) - 1\n",
    "\n",
    "    env, reward, done = env_step(env, action)\n",
    "    print_board(env.board)\n",
    "    if done: break\n",
    "\n",
    "    if player_2_ai:\n",
    "        # you can give it more simulations to make it stronger\n",
    "        action = run_mcts(key, env, 1000).action_weights.argmax().item()\n",
    "    else:\n",
    "        action = int(input()) - 1\n",
    "\n",
    "    env, reward, done = env_step(env, action)\n",
    "    print_board(env.board)\n",
    "    if done: break\n",
    "\n",
    "players = {\n",
    "    1: \"[green]X[/green]\",\n",
    "    -1: \"[red]O[/red]\",\n",
    "}\n",
    "print(f\"Winner: {players[env.player.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = jax.vmap(env_reset)(jax.numpy.arange(128))\n",
    "rng_keys = jax.random.split(jax.random.PRNGKey(0), 128)\n",
    "policy_outputs = jax.vmap(run_mcts, in_axes=(0, 0, None))(rng_keys, envs, 1000)\n",
    "policy_outputs.action_weights.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At `10_000` simulations, the agent was able to beat all online Connect 4 bots I could find."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mctx-classic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
